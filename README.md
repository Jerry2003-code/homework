# GPT-2 ä¸­æ–‡å¯¹è¯æ¨¡å‹å®ç°ä¸è®­ç»ƒ

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![CUDA](https://img.shields.io/badge/CUDA-11.8+-green.svg)](https://developer.nvidia.com/cuda-toolkit)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

**è¯¾ç¨‹ä½œä¸šé¡¹ç›®** - åŸºäºPyTorchä»é›¶å®ç°GPT-2æ¨¡å‹ï¼Œå¹¶è®­ç»ƒä¸­æ–‡å¯¹è¯ç³»ç»Ÿã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- **Python**: 3.8+ (æ¨è3.9æˆ–3.10)
- **PyTorch**: 2.0+ (æ”¯æŒCUDA 11.8+)
- **å†…å­˜**: 8GB+ RAM (æ¨è16GB+)
- **GPU**: NVIDIA GPU (å¯é€‰ï¼Œç”¨äºåŠ é€Ÿè®­ç»ƒ)

### å®‰è£…ä¾èµ–

```bash
# å…‹éš†é¡¹ç›®
git clone <your-repo-url>
cd GPT-2-Model-Implementation

# å®‰è£…ä¾èµ–åŒ…
pip install -r requirements.txt
```

### è¿è¡Œæ­¥éª¤

#### æ–¹æ³•ä¸€ï¼šä¸€é”®è¿è¡Œ
```bash
python scripts/quick_start.py
```

#### æ–¹æ³•äºŒï¼šåˆ†æ­¥æ‰§è¡Œ
1. **æ•°æ®é¢„å¤„ç†**
```bash
python scripts/process_data.py
```

2. **æ¨¡å‹è®­ç»ƒ**
```bash
python scripts/optimized_train.py
```

3. **æ¨¡å‹æµ‹è¯•**
```bash
python scripts/demo.py
```

## ğŸ“Š å®éªŒç»“æœ

### æ¨¡å‹æ€§èƒ½æŒ‡æ ‡
- **æ¨¡å‹å¤§å°**: 159.69 MB
- **å‚æ•°æ•°é‡**: 41,860,830 ä¸ª
- **è¯æ±‡è¡¨å¤§å°**: 1,758 ä¸ªå­—ç¬¦
- **è®­ç»ƒæ•°æ®**: 3,339 æ¡å¯¹è¯

### è®­ç»ƒæ€§èƒ½
- **è®­ç»ƒæ—¶é—´**: ~5åˆ†é’Ÿ (RTX 4060 Laptop GPU)
- **æ¨ç†é€Ÿåº¦**: ~4ms (å•æ¬¡ç”Ÿæˆ)
- **å†…å­˜å ç”¨**: ~2GB (è®­ç»ƒæ—¶)
- **æœ€ç»ˆæŸå¤±**: 1.10 (5ä¸ªepochs)

### æ¨¡å‹æ¶æ„å‚æ•°
- **å±‚æ•°**: 12å±‚Transformerè§£ç å™¨
- **æ³¨æ„åŠ›å¤´æ•°**: 12ä¸ª
- **éšè—ç»´åº¦**: 768
- **æœ€å¤§åºåˆ—é•¿åº¦**: 1024

## ğŸ“ˆ è®­ç»ƒé…ç½®

### è¶…å‚æ•°è®¾ç½®
- **Batch Size**: 16
- **Learning Rate**: 5e-4
- **Epochs**: 10
- **Optimizer**: AdamW
- **Weight Decay**: 0.01
- **Gradient Clipping**: 1.0

## ğŸ¯ ä»£ç ç¤ºä¾‹

### æ¨¡å‹åŠ è½½å’Œæ¨ç†
```python
from src.gpt_model import GPT
import torch
import json

# åŠ è½½è¯æ±‡è¡¨
with open('data/dict_datas.json', 'r', encoding='utf-8') as f:
    dict_datas = json.load(f)
word2id = dict_datas["word2id"]
id2word = dict_datas["id2word"]

# åŠ è½½æ¨¡å‹
model = GPT()
model.load_state_dict(torch.load('models/GPT2.pt'))
model.eval()

# ç”Ÿæˆå›å¤
def generate_response(input_text, max_length=50):
    input_ids = [word2id.get(char, word2id["<unk>"]) for char in input_text]
    input_tensor = torch.tensor([input_ids], dtype=torch.long)
    
    with torch.no_grad():
        for _ in range(max_length):
            outputs, _ = model(input_tensor)
            next_token = torch.argmax(outputs[:, -1, :], dim=-1)
            input_tensor = torch.cat([input_tensor, next_token.unsqueeze(1)], dim=1)
            
            if next_token.item() == word2id["<sep>"]:
                break
    
    # è½¬æ¢å›æ–‡æœ¬
    generated_ids = input_tensor[0].cpu().tolist()
    generated_text = ''.join([id2word[id] for id in generated_ids[len(input_ids):] 
                             if id != word2id["<pad>"]])
    
    return generated_text.replace('<sep>', '').strip()

# ä½¿ç”¨ç¤ºä¾‹
response = generate_response("ä½ å¥½")
print(f"å›å¤: {response}")
```

## ğŸ“ æ•°æ®æ ¼å¼è¯´æ˜

### è®­ç»ƒæ•°æ®æ ¼å¼
æœ¬é¡¹ç›®ä½¿ç”¨é—®ç­”å¯¹æ ¼å¼çš„ä¸­æ–‡å¯¹è¯æ•°æ®ï¼š

```
ä½ å¥½
ä½ å¥½ï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼

ä½ å«ä»€ä¹ˆåå­—ï¼Ÿ
æˆ‘å«AIåŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ã€‚

ä½ èƒ½åšä»€ä¹ˆï¼Ÿ
æˆ‘å¯ä»¥å›ç­”å„ç§é—®é¢˜ï¼Œå¸®åŠ©æ‚¨è§£å†³ç–‘æƒ‘ã€‚
```

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

## ğŸ™ è‡´è°¢

- [OpenAI](https://openai.com/) - GPT-2åŸå§‹è®ºæ–‡å’Œæ¶æ„è®¾è®¡
- [PyTorch](https://pytorch.org/) - æ·±åº¦å­¦ä¹ æ¡†æ¶
- [ViudiraTech](https://github.com/ViudiraTech) - é¡¹ç›®åŸºç¡€ä»£ç 
- è¯¾ç¨‹è€å¸ˆå’ŒåŒå­¦ä»¬çš„æŒ‡å¯¼ä¸å¸®åŠ©

ğŸ“ **ä½œä¸šè¯´æ˜**: æœ¬é¡¹ç›®ä¸ºè¯¾ç¨‹ä½œä¸šï¼Œå®Œæ•´å®ç°äº†GPT-2æ¨¡å‹æ¶æ„å¹¶è®­ç»ƒäº†ä¸­æ–‡å¯¹è¯ç³»ç»Ÿã€‚
